# linear-regression-scratch — 练习解答

1. 如果我们将权重初始化为零，会发生什么？算法仍然有效吗？
   - 对于线性回归（单线性层，含偏置），将权重和偏置初始化为 0 不会造成“对称性破坏”的问题，梯度通常非零，参数会被推动离开 0 并收敛到最小二乘解，算法依然有效。对于深层网络，零初始化会让同层神经元获得完全相同的梯度而无法打破对称，训练会失败，但在线性回归这里没有这个隐患。

2. 假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗？
   - 可以。欧姆定律 V = I·R（或考虑偏置 V = I·R + b）是线性/仿射关系。定义损失函数（如均方误差），利用自动微分求梯度并用（小批量）随机梯度下降即可学习电阻 R（以及可能的偏置 b）。

3. 能基于普朗克定律使用光谱能量密度来确定物体的温度吗？
   - 可以。普朗克定律给出谱辐亮度 L(λ; T) 与温度 T 的关系。给定多波长的观测值，可构建误差函数 ∑‖L̂(λ_i; T, 其它参数) − L_obs(λ_i)‖²，使用自动微分对 T（及发射率、尺度因子等）做非线性最小二乘拟合即可估计温度。需注意：发射率未知、噪声与仪器响应会带来偏差，通常要同时估计这些参数或做适当校准与正则化。

4. 计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？
   - 问题：
     - 计算与内存开销大（需要保存更大的计算图）。
     - 数值不稳定（高阶导数更易出现爆炸/消失）。
     - 框架默认不会保留构建二阶导的计算图，需要特殊设置。
   - 解决：
     - 使用 Hessian-向量积（HVP）等高效技巧，而非显式 Hessian。
     - 在 PyTorch 中用 `autograd.grad(..., create_graph=True)` 以构建二阶所需的图；按需 `retain_graph=True`。
     - 采用双精度、梯度裁剪、正则化与良好缩放以改善数值稳定性；用检查点节省内存。

5. 为什么在 `squared_loss` 函数中需要使用 `reshape`？
   - 为将 `y_hat`、`y` 调整为形状一致的列向量，保证逐元素运算与批量维度对齐，避免由广播导致的形状不匹配或无意的维度扩展，从而在求和/求均值时得到预期的标量损失。

6. 尝试使用不同的学习率，观察损失函数值下降的快慢。
   - 经验现象：学习率过小（如 1e-4）收敛很慢；适中（如 1e-2 ~ 1e-1）下降快且稳定；过大可能振荡或发散。在线性回归的凸二次目标下，存在一个稳定区间，超出上限就会不稳定。

7. 如果样本个数不能被批量大小整除，`data_iter` 函数的行为会有什么变化？
   - 本节实现中，迭代器对索引做随机打乱，并以步长为 `batch_size` 切片。当最后一段长度不足一个完整批次时，仍会产出一个“较小批次”（大小为 `num_examples % batch_size`）。因此每个 epoch 的最后一个批次可能比其他批次小；不会丢弃样本。

